PHYSICS-CONSTRAINED SPLIT-SPLINE J-V CURVE RECONSTRUCTION FOR PEROVSKITE SOLAR CELLS
CONDENSED IMPLEMENTATION PLAN (AS-BUILT PIPELINE)

PROBLEM STATEMENT
Objective: Predict full J-V curves (45 voltage-current pairs) for perovskite solar cells from 31 material parameters with >99.9% accuracy. Challenge: Direct MLP prediction ignores physical boundaries (J cannot exceed Jsc, V cannot exceed Voc) and produces non-monotonic artifacts at the maximum power point (MPP) "knee". Solution: Multi-head neural network that explicitly predicts physical anchors (Jsc, Voc, Vmpp, Jmpp) with hard constraint projection, then reconstructs curve shape using split PCHIP interpolation at MPP to decouple transport-limited (Region 1: 0→Vmpp) from recombination-limited physics (Region 2: Vmpp→Voc).

DATA & PHYSICS CONTEXT
File: `data.py` handles loading and `config.py` defines grid/constants.
Input: 31 COMSOL-simulated material parameters (layer thicknesses, mobilities, carrier lifetimes, doping, work functions, recombination coefficients).
Curves: 45-point J-V characteristics on non-uniform voltage grid [0, 0.1, 0.2, 0.3, 0.4, 0.425, 0.45, ..., 1.4V] totaling 45 evaluation points (`V_GRID` in `config.py`).
Physics features: 71 engineered features computed in `features.py:compute_all_physics_features` from raw params. Includes debye_length, diffusion_length, mobility_ratios, dos_ratios, energy_differences, generation_rate, recombination_timescales, field_strengths, einstein_relations, ideality_factor_proxies, plus thermodynamic ceilings (jsc_ceiling from q×G×L, voc_ceiling from Shockley-Queisser).
Feature validation: Pearson correlation analysis implemented (`features.py:validate_physics_features`) to drop weak features (|r|<0.3 vs targets). Masking logic is in `train.py:ScalarPredictorPipeline._apply_feature_mask`.

IMPLEMENTED ARCHITECTURE (AS-BUILT)
File: `models/voc_nn.py`.
UnifiedSplitSplineNet: Single end-to-end differentiable PyTorch model replacing previous scalar-only cascade (VOC NN + LightGBM). Input: 102 features (31 raw + 71 physics, normalized independently). Backbone: Shared representation [512→256→128] with BatchNorm, SiLU activation, dropout 0.15-0.2. Three parallel output heads:
  Head A (Anchors): Predicts 4 scalars [Jsc, Voc, Vmpp, Jmpp] using `physics_projection` for hard constraints.
  Head B (Region 1 Control): Predicts 6 normalized points (sigmoid 0-1) for curve segment 0→Vmpp (series resistance regime).
  Head C (Region 2 Control): Predicts 6 normalized points (sigmoid 0-1) for curve segment Vmpp→Voc (recombination regime).

HARD CONSTRAINT PROJECTION (CRITICAL REVISION FROM ORIGINAL PLAN)
File: `models/voc_nn.py` function `physics_projection`.
Original plan used Softplus/sigmoid activations (soft constraints allowing violations). Implemented: Differentiable clamping via torch.clamp ensuring:
  Jsc > 1e-6 (strict positivity)
  Voc > 1e-6 (strict positivity)
  1e-6 < Vmpp < Voc - 1e-6 (strict ordering)
  1e-6 < Jmpp < Jsc - 1e-6 (strict ordering)
Guarantees physical validity by construction at machine precision, eliminating probabilistic constraint satisfaction. Projection occurs AFTER head output, preserves gradients for backprop.

SPLIT-SPLINE RECONSTRUCTION LAYER (DIFFERENTIABLE PCHIP)
File: `models/reconstruction.py`.
Algorithm: `build_knots` constructs monotonic voltage-current knot sequences for each region using cumulative control point scaling (revised from linear scaling to enforce monotonicity).
Region 1: 8 knots from (V=0, J=Jsc) anchor through 6 interior points to (V=Vmpp, J=Jmpp) anchor.
Region 2: 8 knots from (V=Vmpp, J=Jmpp) anchor through 6 interior points to (V=Voc, J=0) anchor.
Interpolation: `pchip_interpolate_batch` in `models/reconstruction.py`. Fritsch-Carlson PCHIP (monotonic cubic) implemented in PyTorch for differentiability, guarantees no oscillations within each region.
Stitching: Curves evaluated on v_grid, split at Vmpp, concatenated.
Tail handling: Soft penalty L_tail = relu(J[v>Voc]).mean() with weight ≥1.0 enforces J→0 beyond Voc during training; hard clamp applied only at inference (function `reconstruct_curve(..., clamp_voc=True)`).
Monotonicity validation: Optional checks assert j_knots[i] > j_knots[i+1] for all knots (can disable for speed).

LOSS FUNCTION (KENDALL MULTI-TASK + CONTINUITY)
File: `train.py` class `MultiTaskLoss`.
Original plan: Fixed weights L_total = λ1×L_anchors + λ2×L_curve (arbitrary, causes task interference). Implemented: MultiTaskLoss with learnable log-variances log_sigma_anchor, log_sigma_curve (trainable parameters). Formula: L = L_anchor/(2σ_a²) + log(σ_a) + L_curve/(2σ_c²) + log(σ_c) where L_anchor = MSE(pred_anchors, true_anchors) on 4 scalars, L_curve = MSE(reconstructed_45pts, true_45pts). Automatic balancing eliminates hyperparameter grid search; network learns optimal task weighting during training.
Continuity regularization: `models/reconstruction.py:continuity_loss`. Additional term 0.1 × continuity_loss enforces C1 continuity (value + derivative match) at Vmpp split point using finite differences with local grid spacing: L_cont = (J1(Vmpp) - J2(Vmpp))² + (dJ1/dV|Vmpp - dJ2/dV|Vmpp)². Total loss: L_total = L_weighted + λ_cont×L_cont.

TRAINING PIPELINE
File: `train.py` class `ScalarPredictorPipeline`.
Data retention: Full 45-point curves stored in splits (train/val/test dictionaries contain 'curves' key, shape (N,45)). Voltage grid v_grid (45,) stored in model checkpoint (`models/configs.json`) for inference consistency.
Normalization: raw_params and physics_features normalized independently using per-split statistics (mean/std stored in checkpoint `models/normalization.json`). Curves remain unnormalized for loss computation.
Training loop: `train_curve_model()` enabled via `--train-curves` flag. Optimizer: AdamW with lr 1e-3, batch size up to 4096 (GPU memory permitting). Early stopping: patience=15 on validation curve MSE. Checkpointing: Saves model_state, optimizer_state, epoch, v_grid, feature_mask (if weak features dropped), normalization stats (mean_raw, std_raw, mean_phys, std_phys). Training time: ~50-200 epochs depending on dataset size, convergence tracked via logged sigma values (check if one task dominates).

CVAE BASELINE (IMPLEMENTED FOR COMPARISON)
File: `models/cvae.py`.
Conditional Variational Autoencoder: --train-cvae flag trains comparison model. Architecture: Encoder (curve + params → latent_dim=16 via mu/logvar), Decoder (latent + params → reconstructed curve). Loss: cvae_loss = MSE_reconstruction + β×KL_divergence with β=0.001 tuned. Purpose: Demonstrate split-spline method outperforms latent-space approaches on constraint satisfaction (CVAE can produce J>Jsc, Vmpp>Voc) and FF precision. Evaluation: Same train/val/test split as split-spline model, metrics logged for comparison table (MSE, anchor MAE, FF MAPE, constraint violations).

EVALUATION METRICS (IMPLEMENTED)
File: `train.py` method `evaluate_curve_model` & `evaluate_cvae`.
Full-curve MSE: Mean squared error over all 45 voltage points. Region-wise MSE: Separate error for Region 1 (v≤Vmpp_pred) and Region 2 (v>Vmpp_pred) to assess split effectiveness. Anchor precision: MAE for Jsc (mA/cm²), Voc (mV), Vmpp (mV), Jmpp (mA/cm²). Fill Factor error: MAPE(FF) where FF = (Vmpp×Jmpp)/(Voc×Jsc), most critical metric for solar cell evaluation, target <5%. Constraint violation counts: Per-batch counts of jsc<0, voc<0, vmpp invalid, jmpp invalid, J>Jsc anywhere on curve; should be zero with hard projection. Logged during validation every epoch, reported on test set at completion.

INFERENCE PIPELINE (INTEGRATED)
File: `inference.py`.
Command: `python inference.py --predict-curve --input params.csv --output curve_predictions.csv` (uses `slurm_curve_pipeline.sh` for HPC).
Workflow: `ScalarPredictor` loads checkpoint containing model weights (`voc_nn.pt`, `curve_model.pt`, etc.), v_grid, feature_mask, normalization stats. Compute 71 physics features from input 31 params (apply feature mask if weak features were dropped). Normalize inputs using stored mean/std. Forward pass: anchors, ctrl1, ctrl2 = model(raw_params, physics_features). Reconstruct curve: j_curve = reconstruct_curve(anchors, ctrl1, ctrl2, v_grid).
Uncertainty (optional --uncertainty flag): `models/voc_nn.py:predict_with_uncertainty`. MC-Dropout with n_samples=100, returns mean and std per voltage point.
Output: CSV with columns [V0, J0, V1, J1, ..., V44, J44] and separate v_grid CSV.

VALIDATION RESULTS (WHAT'S VERIFIED)
Data pipeline correct, curves retained, constraints enforced, monotonicity guaranteed, continuity achieved, MSE <0.001 on val set.
Comparison: CVAE baseline trainable and evaluable, constraint violations occur (proves split-spline advantage).

WHAT REMAINS (NEXT STEPS FOR PUBLICATION)
Ablation studies: No-split (single spline), no-continuity, no-anchors, no-physics-features, direct-MLP baselines. Runner script needed to loop configs, train each, compile results table showing MSE/FF_MAPE/violations per ablation. Target: Demonstrate split improves FF error by ≥30% vs single spline.
OOD evaluation: Helper `test_ood_generalization` in `train.py`. Bandgap-based extrapolation (train E_g<1.6eV, test E_g>1.8eV), leave-one-material-out if material labels exist. Requires identifying bandgap column or computing from params. Target: Show OOD error <2× interpolation error.
Uncertainty calibration: Run MC-Dropout on test set, verify 95% prediction intervals contain true curves for 90-97% of samples (well-calibrated). Generate plots with shaded uncertainty bands.
Automated test suite: Unit tests for constraint satisfaction, monotonicity, continuity, anchor precision, FF error. Integrate into CI/CD (GitHub Actions) for regression testing.
Experimental validation: Acquire 10-20 experimental J-V curves from literature (NREL database, published papers), test trained model, report accuracy separately for COMSOL vs experimental. Add paper disclaimer: "Validated on COMSOL; experimental validation ongoing."
Sensitivity analysis: Compute Jacobians ∂Jsc/∂param, ∂Voc/∂param via autograd, validate signs align with semiconductor physics, compare to CVAE Jacobians for interpretability claims.

PUBLICATION POSITIONING
Novelty: Explicit split at MPP for PSC biphasic physics (not generic autoencoder), hard constraint projection (not soft penalties), end-to-end differentiable PCHIP (not post-hoc curve fitting), automatic loss weighting (not manual λ tuning). Advantages vs competitors: CVAE cannot guarantee J<Jsc (your method does), direct MLP ignores MPP transition (your method decouples), LightGBM cascade not end-to-end differentiable (your method is). Target metrics for acceptance: Full-curve MSE <0.001, FF MAPE <3%, zero constraint violations, OOD error ratio <2×, ablation shows split improves FF by >30%. Target journals: Solar Energy Materials & Solar Cells (domain-focused), ACS Applied Energy Materials (computational-friendly), Journal of Materials Chemistry A (ML+materials). Reviewer defense: Comparison table (split-spline vs CVAE vs direct-MLP), ablation study proving each component necessary, OOD tests showing extrapolation capability, uncertainty quantification for user trust.

CURRENT STATUS SUMMARY
Working: Hard projection, multi-head architecture, PCHIP reconstruction, Kendall loss, continuity enforcement, CVAE baseline, feature validation, curve inference, evaluation metrics.
Validated: Data pipeline correct, curves retained, constraints enforced, monotonicity guaranteed, continuity achieved, MSE <0.001 on val set.
Missing: Ablation runner, OOD test execution, uncertainty calibration demo, automated test suite, experimental validation.
Timeline to publication: 3-4 weeks for ablations/OOD/uncertainty, 2 weeks for paper writing/figures, 1 week for final polishing = 6-7 weeks to submission if executed systematically.

KEY IMPLEMENTATION DETAILS FOR REPLICATION
Voltage grid construction: `config.py` hardcodes `V_GRID` (45 points).
Control point scaling: `models/reconstruction.py:build_knots` ensures monotonic decrease.
PCHIP slopes: Fritsch-Carlson method with safe division (add 1e-8 to dx denominators) in `models/reconstruction.py`.
Continuity finite differences: Use local spacing (v_grid[i+1]-v_grid[i-1])/2 not global step.
Kendall initialization: log_sigma = torch.zeros(1) → σ=1.0 initially (balanced start).
Feature mask persistence: Store boolean array in checkpoint, apply BEFORE physics computation at inference.
Batch compatibility: All operations support batch_first convention, tested with batch_size=1 and batch_size=4096.

END OF CONDENSED PLAN